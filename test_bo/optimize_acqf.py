#!/usr/bin/env python3
# Copyright (c) Stanford University and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

r"""Optimize acquisition function."""

import torch
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

def optimize_acqf(
    config,
    generator,
    embedder,
    buffer,
    reward_model,
    oracle,
    generator_optim,
    generator_scaler,
    iteration,
    *args,
    **kwargs,
):
    acqf_loss = []
    entropies_list = []
    
    if config.use_TS:
        TS_idx = torch.randint(high=config.n_samples, size=(1,))
        TS_idx = TS_idx.item()
        print("IDX", TS_idx)
    else:
        TS_idx = None

    # reset generator parameters
    # if layer is not called g2e_transl_layer in the generator
    for name, layer in generator.named_children():
        if name != 'g2e_transl_layer':
            if hasattr(layer, 'reset_parameters'):
                layer.reset_parameters()

    # batch_size = config.batch_size
    batch_size = 10

    for i in tqdm(range(config.generator_iter)):
        generator_optim.zero_grad()
        # with torch.cuda.amp.autocast():
        # Get prompt from user and reply
        prompt = oracle.get_prompt(
            # batch_size=config.batch_size,
            batch_size=batch_size,
            prompt_length=config.prompt_length,
        )
        # >>> prompt_len x batch_size [x vocab_size_generator]
        
        outputs, outputs_probs, entropies, log_probs = generator.generate(
            input_tensors=prompt,
            n_restart=config.n_restart,
            q=config.q,
            max_length=config.max_length,
        )
        # >>> batch_size x n_restart x q
        # ... x max_length [x vocab_size_generator]

        outputs = torch.randint_like(outputs, high=config.vocab_size_generator)
        # >>> batch_size x n_restart x q x max_length
        dist = torch.distributions.Categorical(probs=outputs_probs[..., -1:, :])
        # >>> batch_size x n_restart x q x vocab_size_generator

        log_probs = dist.log_prob(outputs[..., -1:])
    
        entropies_list.append(- entropies.sum(-1).mean().item())

        if config.use_dynamic_gradient:
            embed_outputs = embedder(
                sentence=outputs,
                g2e_transl=generator.g2e_transl,
            )
            # >>> batch_size x n_restart x q x embed_dim
            
            losses = reward_model.posterior_function(embed_outputs).squeeze(-1)
            # >>> n_samples x batch_size x n_restart x q
            
            entropies = entropies.sum(-1)
            # >>> batch_size x n_restart x q
            
            entropies = entropies[None, ...].expand_as(losses)
            # >>> n_samples x batch_size x n_restart x q
            
            losses = torch.sigmoid(losses) 
            acqf_loss.append( - (losses.softmax(-1)*losses).sum(-1).mean().item())
            
            losses = losses + entropies * config.entropy_weight
            # >>> n_samples x batch_size x n_restart x q
            
        else:
            embed_outputs = embedder(
                sentence=outputs,
                g2e_transl=generator.g2e_transl,
            )
            # >>> batch_size x n_restart x q x embed_dim

            reward = reward_model.posterior_function(
                input=embed_outputs, TS_idx=TS_idx,
            ).squeeze(-1).detach()
            reward = torch.sigmoid(reward)
            # >>> [n_samples|¬use_TS] x batch_size x n_restart x q

            #======================================================================
            # TESTING
            #======================================================================
            # outputs_comp_probs = outputs_probs[..., -1:, :]
            # # outputs_comp = torch.randint_like(outputs[..., -1:], high=config.vocab_size_generator)
            
            # # For each prompt, we find the completion with the highest reward
            # # Then we use that completion for calculating the log_prob
            # # This is because we want to optimize the completion generated by the generator
            # # to be the one with the highest reward
            
            # comp = torch.arange(0, config.vocab_size_generator)[None, :, None].expand(batch_size, -1, -1)
            # comp = comp.to(outputs.device, outputs.dtype)
            # # >>> batch_size x vocab_size_generator x 1
            
            # prompt = prompt.permute(1, 0)[:, None, :].expand(-1, config.vocab_size_generator, -1)
            # # >>> batch_size x vocab_size_generator x prompt_length
            
            # promptAcomp = torch.cat([prompt, comp], dim=-1)
            # # >>> batch_size x vocab_size_generator x max_length
            
            # embed_promptAcomp = embedder(
            #     sentence=promptAcomp,
            #     g2e_transl=generator.g2e_transl,
            # )
            # reward2learn = reward_model.posterior_function(
            #     input=embed_promptAcomp, TS_idx=TS_idx,
            # ).squeeze(-1).detach()
            # # >>> batch_size x vocab_size_generator
            
            # prefer_idx = reward2learn.argmax(-1)
            # batch_idx = torch.arange(batch_size)
            
            # outputs_comp = comp[batch_idx, prefer_idx]
            # # >>> batch_size x comp_length
            # outputs_comp = outputs_comp[:, None, None, :].expand(-1, config.n_restart, config.q, -1)
            # # >>> batch_size x n_restart x q x comp_length
                
            # dist = torch.distributions.Categorical(probs=outputs_comp_probs)
            # # >>> batch_size x n_restart x q x comp_length x vocab_size_generator
            # log_probs = dist.log_prob(outputs_comp)
            #======================================================================
            
            log_probs = log_probs.sum(-1)
            entropies = entropies.sum(-1)
            # >>> batch_size x n_restart x q

            if not config.use_TS:
                log_probs = log_probs[None, ...].expand_as(reward)
                entropies = entropies[None, ...].expand_as(reward)
                # >>> n_samples x batch_size x n_restart x q

            losses = reward * log_probs 
            acqf_loss.append( - (losses.softmax(-1)*losses).sum(-1).mean().item())
            
            losses = losses + entropies * config.entropy_weight
            # >>> [n_samples|¬use_TS] x batch_size x n_restart x q

        losses = - losses.mean(-1)
        # losses = losses.min(-1).values
        # losses = - (losses.softmax(-1)*losses).sum(-1)
        # >>> [n_samples|¬use_TS] x batch_size x n_restart

        if config.use_dynamic_gradient or (not config.use_TS):
            losses = losses.mean(0)
            # >>> batch_size x n_restart

        loss = losses.mean()
        
        # generator_scaler.scale(loss).backward()
        # generator_scaler.step(generator_optim)
        # generator_scaler.update()
        loss.backward()
        generator_optim.step()
        
        if i % (config.generator_iter//5) == 0:
            print(f"Acqf loss {i}/{config.generator_iter}: {loss.item():.3f}")

    # select pair with lowest loss
    i = losses.argmin(-1)
    j = torch.arange(batch_size)
    # >>> batch_size
    
    # Smooth covolution with window size of 10
    # using np.convolve
    smth_window = 10
    smooth_acqf_loss = np.convolve(
        acqf_loss, 
        np.ones((smth_window,))/smth_window, 
        mode='valid'
    )
    smooth_entropies_list = np.convolve(
        entropies_list,
        np.ones((smth_window,))/smth_window,
        mode='valid'
    )
    
    plt.figure()
    plt.plot(acqf_loss, label=r"$-R$")
    # plt.plot(smooth_acqf_loss, label="acqf loss")
    plt.plot(entropies_list, label=r"$-\mathcal{H}$")
    # plt.plot(smooth_entropies_list, label="entropy")
    plt.hlines(-1.0, 0, len(acqf_loss), linestyles="dashed")
    plt.hlines(-np.log(config.vocab_size_generator), 0, len(acqf_loss), linestyles="dashed")
    plt.ylim(-6, 10)
    plt.legend()
    plt.savefig(f"results/{config.exp_id}/acqf_loss{iteration}.pdf")
    plt.close()
    
    return outputs[j, i], embed_outputs[j, i]
